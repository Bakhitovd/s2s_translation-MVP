# s2s_translation-MVP

# MVP Dev Doc — **Canary AST → Silero TTS** (Local, Windows + Docker + RTX A4000)

This guide delivers a **local, web-based** EN↔RU speech→speech MVP that runs **fully offline**:
**AST**: `nvidia/canary-1b-v2` (speech→translated text) → **TTS**: Silero (text→speech).
Frontend: Chrome (mic/loopback) → WebSocket → FastAPI (Python) → GPU inference → audio back.

Canary v2 supports **ASR + AST** for 25 EU languages (incl. **en, ru**), accepts **16 kHz mono** WAV/FLAC, runs on **Ampere** (A4000 OK), and is **CC-BY-4.0**. It exposes a NeMo API: `ASRModel.from_pretrained(...).transcribe(source_lang=..., target_lang=...)`. Minimum **\~6 GB RAM** to load. ([Hugging Face][1])
Silero TTS is an **offline** PyTorch Hub model (one-line load), with **RU (6 voices) + EN** speakers, **8/16 kHz**, “no GPU required,” minimal deps. ([PyTorch][2])

---

## 0) What you get

* **Bidirectional EN↔RU** speech→speech using a **single model for AST** (no separate ASR). ([Hugging Face][1])
* **Chrome-only** web client (mic capture + playback).
* **I/O sources**: Mic, or system/meeting audio via **loopback** (Stereo Mix / VB-Audio Cable). Chrome cannot grab system audio directly via `getUserMedia`; use loopback devices. ([Stack Overflow][3], [VB-Audio][4])
* **Docker Desktop + GPU** on Windows/WSL2 using **NVIDIA Container Toolkit**. ([Docker Documentation][5], [NVIDIA Docs][6])

---

## 1) Prereqs (Windows + Docker + GPU)

1. **Update** NVIDIA driver + Windows 10/11; enable **WSL2**.
2. Install **Docker Desktop**; enable **GPU** support (WSL2 GPU paravirtualization). Test with `docker run --gpus all nvidia/cuda:12.2.0-base nvidia-smi`. ([Docker Documentation][5])
3. (Optional) Install **VB-Audio Cable** for loopback (system/Zoom/Meet/Teams/YouTube → “virtual mic”). ([VB-Audio][7])

---

## 2) Stack at a glance

* **AST**: `nvidia/canary-1b-v2` (NeMo). Usage:

  ```python
  from nemo.collections.asr.models import ASRModel
  m = ASRModel.from_pretrained(model_name="nvidia/canary-1b-v2")
  out = m.transcribe([wav_path], source_lang="en", target_lang="ru")  # also ru->en
  print(out[0].text)
  ```

  Supports **translating with timestamps**; input **16 kHz mono**. ([Hugging Face][1])

* **TTS (local)**: **Silero TTS** (PyTorch Hub) — EN voice `lj_16khz`, RU voices (e.g., `baya`).

  ```python
  import torch
  lang, spk = ('ru','baya')  # or ('en','lj_16khz')
  model, symbols, sr, _, apply_tts = torch.hub.load('snakers4/silero-models','silero_tts',
                                                    language=lang, speaker=spk)
  audio = apply_tts(texts=[text], model=model, sample_rate=sr, symbols=symbols)
  ```

  **One-line usage**, **8/16 kHz**, offline. ([PyTorch][2])

* **Web/API**: **FastAPI** + **WebSockets** (duplex streaming). ([FastAPI][8])

---

## 3) Project layout

```
mvp/
  frontend/
    index.html
    app.js
  server/
    app.py            # FastAPI + WS + pipeline
    canary_ast.py     # NeMo model init + AST
    silero_tts.py     # TTS wrapper
    audio_util.py     # resample (48k→16k), WAV framing
    requirements.txt
  docker/
    Dockerfile
    docker-compose.yml
  docs/
    RUNBOOK.md
    AUDIO_ROUTING.md
```

---

## 4) Dockerization (GPU + PyTorch + NeMo + Silero)

**docker/Dockerfile**

```dockerfile
FROM pytorch/pytorch:2.3.1-cuda12.1-cudnn8-runtime

# System deps
RUN apt-get update && apt-get install -y --no-install-recommends \
    ffmpeg libsndfile1 sox && rm -rf /var/lib/apt/lists/*

# Python deps
WORKDIR /app
COPY server/requirements.txt /app/
RUN pip install --no-cache-dir -r requirements.txt \
    && pip install -U "nemo_toolkit[asr]"  # NeMo (per model card)

# App
COPY server /app/server
COPY frontend /app/frontend

EXPOSE 8000
CMD ["uvicorn", "server.app:app", "--host", "0.0.0.0", "--port", "8000"]
```

**server/requirements.txt**

```
fastapi==0.115.0
uvicorn[standard]==0.30.0
websockets==12.0
soundfile==0.12.1
numpy==1.26.4
scipy==1.13.1
torchaudio==2.3.1
transformers==4.43.3   # (for utils; not used for MT in this AST-first build)
```

**docker/docker-compose.yml**

```yaml
services:
  mvp:
    build: ../
    image: mvp-canary-ast
    ports: ["8000:8000"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    environment:
      - TORCH_CUDA_ARCH_LIST=8.6
    volumes:
      - hf_cache:/root/.cache/huggingface  # cache Canary weights
volumes:
  hf_cache:
```

GPU in Docker Desktop/WSL2: **enable GPU** feature; compose `devices` as above. ([Docker Documentation][5])

---

## 5) Backend (FastAPI + WS + Pipeline)

**server/canary\_ast.py**

```python
import torch
from nemo.collections.asr.models import ASRModel

_model = None

def load_canary():
    global _model
    if _model is None:
        _model = ASRModel.from_pretrained(model_name="nvidia/canary-1b-v2")
        # Canary expects 16k mono WAV/FLAC; we'll feed numpy float32 16k mono arrays.
    return _model

def ast_translate(wav_path_or_array, src_lang:str, tgt_lang:str, timestamps=False):
    m = load_canary()
    if timestamps:
        out = m.transcribe([wav_path_or_array], source_lang=src_lang, target_lang=tgt_lang, return_timestamps=True)
        return out[0]  # .text, .segments
    else:
        out = m.transcribe([wav_path_or_array], source_lang=src_lang, target_lang=tgt_lang)
        return out[0]  # .text
```

Canary v2 usage + 16 kHz mono input + `source_lang/target_lang` come from model card. ([Hugging Face][1])

**server/silero\_tts.py**

```python
import torch

_tts_cache = {}

def load_tts(lang, speaker):
    key = (lang, speaker)
    if key not in _tts_cache:
        model, symbols, sr, example_text, apply_tts = torch.hub.load(
            'snakers4/silero-models', 'silero_tts', language=lang, speaker=speaker
        )
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model = model.to(device)
        _tts_cache[key] = (model, symbols, sr, apply_tts, device)
    return _tts_cache[key]

def synth(text, lang='ru', speaker='baya'):
    model, symbols, sr, apply_tts, device = load_tts(lang, speaker)
    audio = apply_tts(texts=[text], model=model, sample_rate=sr, symbols=symbols, device=device)
    return sr, audio[0]  # numpy float32
```

Silero: **one-line load**, EN+RU voices, 8/16 kHz supported. ([PyTorch][2])

**server/audio\_util.py**

```python
import numpy as np
import soundfile as sf
from scipy.signal import resample_poly

def to_mono16k_float32(pcm_s16_le, in_sr=48000, channels=2):
    x = np.frombuffer(pcm_s16_le, dtype=np.int16).astype(np.float32) / 32768.0
    x = x.reshape(-1, channels).mean(axis=1) if channels > 1 else x
    # resample 48k -> 16k
    y = resample_poly(x, up=1, down=3)
    return 16000, y.astype(np.float32)

def wav_bytes_from_float32(y, sr):
    import io
    buf = io.BytesIO()
    sf.write(buf, y, sr, format='WAV', subtype='PCM_16')
    return buf.getvalue()
```

**server/app.py** (WS endpoint, utterance-based MVP)

```python
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.staticfiles import StaticFiles
from server.audio_util import to_mono16k_float32, wav_bytes_from_float32
from server.canary_ast import ast_translate
from server.silero_tts import synth
import asyncio, time, numpy as np

app = FastAPI()
app.mount("/", StaticFiles(directory="../frontend", html=True), name="static")

@app.websocket("/ws/translate")
async def ws_translate(ws: WebSocket):
    await ws.accept()
    # client sends JSON first: {"src":"auto|en|ru","tgt":"auto|en|ru"}
    cfg = await ws.receive_json()
    mode = cfg.get("mode", "auto")  # "en->ru" / "ru->en" / "auto"
    # accumulate one utterance then process; simple VAD by timeout of silence from client
    buf = bytearray()
    last = time.time()
    try:
        while True:
            msg = await ws.receive()
            if "bytes" in msg:
                buf.extend(msg["bytes"])
                last = time.time()
            elif "text" in msg and msg["text"] == "__flush__":
                # finalize utterance
                await handle_utterance(ws, buf, mode)
                buf = bytearray()
            # idle flush (no bytes for 900 ms)
            if len(buf) > 0 and time.time() - last > 0.9:
                await handle_utterance(ws, buf, mode)
                buf = bytearray()
    except WebSocketDisconnect:
        pass

async def handle_utterance(ws, buf: bytearray, mode: str):
    if not buf:
        return
    sr, x = to_mono16k_float32(bytes(buf), in_sr=48000, channels=2)
    # pick direction (auto: detect by trying EN->RU first; refine as needed)
    src, tgt = ("en","ru") if mode in ("auto","en->ru") else ("ru","en")
    res = ast_translate(x, src_lang=src, tgt_lang=tgt)  # Canary AST
    text = res.text if hasattr(res, "text") else res
    out_sr, y = synth(text, lang=tgt, speaker=("baya" if tgt=="ru" else "lj_16khz"))
    wav = wav_bytes_from_float32(y, out_sr)
    await ws.send_bytes(wav)      # send single WAV blob for this utterance
    await ws.send_json({"src": src, "tgt": tgt, "text": text})
```

FastAPI WS pattern matches docs; this MVP does **utterance flush** by idle timeout or explicit flush. ([FastAPI][8])

---

## 6) Frontend (Chrome)

**frontend/index.html**

```html
<!doctype html><meta charset="utf-8">
<button id="start">Start</button> <button id="stop">Stop</button>
<select id="mode">
  <option value="auto">auto</option>
  <option value="en->ru">en→ru</option>
  <option value="ru->en">ru→en</option>
</select>
<pre id="log"></pre>
<script src="app.js"></script>
```

**frontend/app.js**

```js
const log = (...a)=>document.getElementById('log').textContent += a.join(' ')+'\n';
let ws, rec, audioCtx, srcNode, procNode;

async function start() {
  ws = new WebSocket(`ws://${location.host}/ws/translate`);
  await new Promise(r => ws.addEventListener('open', r, {once:true}));
  ws.send(JSON.stringify({mode: document.getElementById('mode').value}));
  ws.addEventListener('message', async ev => {
    if (typeof ev.data === 'string') {
      const j = JSON.parse(ev.data);
      if (j.text) log(`[${j.src}→${j.tgt}] ${j.text}`);
    } else {
      // audio WAV blob; play
      const blob = new Blob([ev.data], {type:'audio/wav'});
      const url = URL.createObjectURL(blob);
      new Audio(url).play();
    }
  });

  const stream = await navigator.mediaDevices.getUserMedia({audio: {echoCancellation:false}});
  audioCtx = new AudioContext({sampleRate:48000});
  srcNode = audioCtx.createMediaStreamSource(stream);
  const workletUrl = URL.createObjectURL(new Blob([`
    class PCMWorklet extends AudioWorkletProcessor{
      process(inputs){
        const ch=inputs[0][0]; if(!ch) return true;
        const pcm=new Int16Array(ch.length);
        for(let i=0;i<ch.length;i++) pcm[i]=Math.max(-1,Math.min(1,ch[i]))*32767|0;
        this.port.postMessage(pcm.buffer,[pcm.buffer]);
        return true;
      } }
    registerProcessor('pcm-worklet', PCMWorklet);
  `],{type:'application/javascript'}));
  await audioCtx.audioWorklet.addModule(workletUrl);
  procNode = new AudioWorkletNode(audioCtx, 'pcm-worklet');
  procNode.port.onmessage = e => ws.readyState===1 && ws.send(e.data);
  srcNode.connect(procNode);
  log('Started.');
}
function stop() {
  ws && ws.send('__flush__'); // force finalization
  ws && ws.close(); ws=null;
  audioCtx && audioCtx.close();
  log('Stopped.');
}
document.getElementById('start').onclick = start;
document.getElementById('stop').onclick = stop;
```

**System/meeting audio**: In Chrome, choose a **loopback device** (e.g., *Stereo Mix* or **VB-Audio Cable Output**) as the “microphone” in site permissions, so Zoom/YouTube/Teams audio becomes input to the app. ([Stack Overflow][3], [VB-Audio][7])

---

## 7) Runbook

```bash
# build & run
cd docker
docker compose up --build

# open Chrome:
http://localhost:8000   # allow mic access, pick mode, speak
```

**Notes**

* Canary v2 expects **16 kHz mono**; we resample from 48 kHz capture. ([Hugging Face][1])
* GPU is used by NeMo/Canary; Silero can run CPU or CUDA.
* Canary model load needs **\~6 GB RAM**; A4000 (16 GB VRAM) is fine. ([Hugging Face][1])
* If load is slow first time, weights are being downloaded (cached in the compose volume).

---

## 8) Audio routing cheat sheet

* **Zoom/Meet/Teams in → app**: set system playback to **VB-Cable Input**, then choose **VB-Cable Output** (loopback) as the Chrome mic for our app. (VB-Cable manual shows loopback behavior.) ([VB-Audio][4])
* **YouTube tab → app**: simplest is also **VB-Cable** (or Windows *Stereo Mix* if available). Chrome `getUserMedia` cannot capture system output directly. ([Stack Overflow][3])

---

## 9) Performance & streaming tips

* **Chunking policy**: This MVP finalizes on **idle \~900 ms**; for finer latency, enable **timestamps** from Canary and stream translated **segments** to TTS as they arrive. (Model card shows translating with timestamps; long-form inference uses **dynamic chunking with 1 s overlap**.) ([Hugging Face][1])
* **Languages**: Canary supports **EN↔RU** among 25 languages; you can auto-flip target based on detected `source_lang`. ([Hugging Face][1])
* **FastAPI WS** patterns are documented here (disconnect handling, broadcast, etc.). ([FastAPI][8])

---

## 10) Why this stack

* **One model hop** (speech→translated text) ⇒ less plumbing/latency; **local, CC-BY-4.0**; **Ampere-ready**. ([Hugging Face][1])
* **TTS is trivial to wire** and fully offline (RU+EN voices). ([PyTorch][2])
* **Windows + Docker GPU** is officially supported with WSL2. ([Docker Documentation][5])

---

## 11) Cline / VS Code workflow (optional but recommended)

* Add a `.clinerules/` with persistent memory + task templates; keep **tasks** small (frontend WS capture, AST wrapper, TTS wrapper, resampler, Dockerfile). The template approach helps AI agents keep context and follow the plan. ([GitHub][9])

---

## 12) Legal & licensing (POC)

* **Canary-1b-v2** — **CC-BY-4.0** (attribution required in docs/UI). ([Hugging Face][1])
* **Silero TTS** — verify license terms in repo before commercializing; for this **personal POC** it runs locally with PyTorch Hub. ([GitHub][10])

---

### References

* **Canary-1b-v2 model card** (langs, API, 16 kHz input, CC-BY-4.0, HW reqs, dynamic chunking). ([Hugging Face][1])
* **NVIDIA blog (Aug 15 2025)** — Canary v2 release & scope. ([NVIDIA Blog][11])
* **NeMo docs** — `from_pretrained` checkpoints. ([NVIDIA Docs][12])
* **Silero TTS (PyTorch Hub)** — usage, RU/EN voices, 8/16 kHz, offline. ([PyTorch][2])
* **Docker Desktop GPU/WSL2**. ([Docker Documentation][5])
* **Loopback audio** — Chrome system audio via **Stereo Mix / VB-Cable**. ([Stack Overflow][3], [VB-Audio][4])

If you want, I can drop this into a ready-to-run repo skeleton (folders + files above) so you can start coding with Cline immediately.

[1]: https://huggingface.co/nvidia/canary-1b-v2 "nvidia/canary-1b-v2 · Hugging Face"
[2]: https://pytorch.org/hub/snakers4_silero-models_tts/ "Silero Text-To-Speech Models – PyTorch"
[3]: https://stackoverflow.com/questions/36400257/getusermedia-get-computer-audio?utm_source=chatgpt.com "GetUserMedia get computer audio - webrtc"
[4]: https://vb-audio.com/Cable/VBCABLE_ReferenceManual.pdf?utm_source=chatgpt.com "VB-AUDIO CABLE - Reference Manual"
[5]: https://docs.docker.com/desktop/features/gpu/?utm_source=chatgpt.com "GPU support in Docker Desktop for Windows"
[6]: https://docs.nvidia.com/cuda/wsl-user-guide/index.html?utm_source=chatgpt.com "CUDA on WSL User Guide"
[7]: https://vb-audio.com/Cable/?utm_source=chatgpt.com "VB-Audio Virtual Apps"
[8]: https://fastapi.tiangolo.com/advanced/websockets/?utm_source=chatgpt.com "WebSockets - FastAPI"
[9]: https://github.com/Honghe/demo_fastapi_websocket?utm_source=chatgpt.com "Demo FastAPI WebSocket Audio"
[10]: https://github.com/snakers4/silero-models?utm_source=chatgpt.com "snakers4/silero-models"
[11]: https://blogs.nvidia.com/blog/speech-ai-dataset-models/?utm_source=chatgpt.com "Now We're Talking: NVIDIA Releases Open Dataset, Models for Multilingual Speech AI"
[12]: https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/asr/results.html?utm_source=chatgpt.com "Checkpoints — NVIDIA NeMo Framework User Guide"
